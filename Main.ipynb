{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for creating and training the LSTM network models used to generate classical music.\n",
    "\n",
    "Matilda Wikstr√∂m\n",
    "\n",
    "Jesper Larsson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to good stuff!\n",
    "\n",
    "https://github.com/craffel/pretty-midi\n",
    "\n",
    "https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "# \"Our code\"\n",
    "import preprocessing as pp\n",
    "#import generatemidi as gm # Broken file, fix\n",
    "import tokenizer as token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29070\n"
     ]
    }
   ],
   "source": [
    "# Using pretty_midi example\n",
    "fs=30\n",
    "dict_note = {}\n",
    "midi_pretty_format = pretty_midi.PrettyMIDI('test.midi')\n",
    "piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
    "piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
    "dict_note[0] = piano_roll\n",
    "\n",
    "print(len(piano_roll[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the required data create folder structure\n",
    "#Make sure we are in the correct folder\n",
    "\n",
    "assert (os.path.basename(os.getcwd())=='LSTM-MusicGenerator'), \"Wrong working dir\"\n",
    "\n",
    "#Download the MAESTRO Dataset\n",
    "if not os.path.isfile('./maestro-v1.0.0-midi.zip'):\n",
    "    !wget https://storage.googleapis.com/magentadata/datasets/maestro/v1.0.0/maestro-v1.0.0-midi.zip\n",
    "\n",
    "#Check if we have extracted the files\n",
    "if not os.path.isdir('./maestro-v1.0.0-midi'):\n",
    "    !unzip maestro-v1.0.0-midi.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61d6a8a681f4fd9b2fd7e221e4d96fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "list_all_midi = pp.get_list_midi() # \n",
    "sampled_200_midi = list_all_midi[0:100]\n",
    "batch = 1\n",
    "start_index = 0\n",
    "note_tokenizer = token.NoteTokenizer()\n",
    "import pretty_midi\n",
    "\n",
    "for i in tqdm_notebook(range(len(sampled_200_midi))):\n",
    "    dict_time_notes = token.generate_dict_time_notes(sampled_200_midi, batch_song=1, start_index=i, use_tqdm=False, fs=5)\n",
    "    full_notes = token.process_notes_in_song(dict_time_notes)\n",
    "    for note in full_notes:\n",
    "        note_tokenizer.partial_fit(list(note.values()))\n",
    "note_tokenizer.add_new_note('e') # Add empty notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "\n",
    "#unique_notes+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Input dimension = 50\n",
    "#Batch dimension = ? May be one, since we are putting them on \n",
    "#Hidden size, try different versions\n",
    "   \n",
    "input_size = 50\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "is_bidirectional = False\n",
    "#dropout_rate = 0.75\n",
    "dropout_rate = 0\n",
    "batch_size = 1 #96 ### Should be 2085\n",
    "\n",
    "seq_len = 50\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 16\n",
    "BATCH_NNET_SIZE = 96 \n",
    "TOTAL_SONGS = len(sampled_200_midi)\n",
    "FRAME_PER_SECOND = 5\n",
    "\n",
    "\n",
    "unique_notes = note_tokenizer.unique_word #Used in our output layer to map\n",
    "\n",
    "\n",
    "\n",
    "if is_bidirectional:\n",
    "    num_directions = 2\n",
    "else:\n",
    "    num_directions = 1\n",
    "\n",
    "input = torch.randn(1, batch_size, input_size)\n",
    "h0 = torch.randn(num_layers*num_directions, batch_size, hidden_size,dtype=torch.float32)\n",
    "c0 = torch.randn(num_layers*num_directions, batch_size, hidden_size,dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class our_LSTM(torch.nn.Module):\n",
    "    def __init__(self, h0, c0 ):\n",
    "        super().__init__()\n",
    "        self.h0 = h0\n",
    "        self.c0 = c0\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout_rate, bidirectional=is_bidirectional)\n",
    "        self.fc = torch.nn.Linear(num_directions * hidden_size, unique_notes)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inp, hn,cn):\n",
    "        out, (hn, cn) = self.lstm(inp, (hn, cn))\n",
    "        out = self.fc(out)\n",
    "        #out = self.softmax(out)\n",
    "        out = out.permute(0,2,1)\n",
    "        #out = out[:][:][0]\n",
    "        \n",
    "        return out, hn, cn\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '62,65']\n",
      "['61,63,65,67']\n",
      "(2085, 50)\n",
      "(2085, 1)\n",
      "HEllo\n",
      "(50,)\n",
      "(1,)\n",
      "torch.Size([2085, 1, 50])\n"
     ]
    }
   ],
   "source": [
    "## DON'T RUN THIS CELL ##\n",
    "\n",
    "seq_len = 50\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 16\n",
    "BATCH_NNET_SIZE = 96\n",
    "TOTAL_SONGS = len(sampled_200_midi)\n",
    "FRAME_PER_SECOND = 5\n",
    "\n",
    "batch_song = BATCH_SONG\n",
    "frame_per_second = FRAME_PER_SECOND\n",
    "inputs_nnet_large, outputs_nnet_large = token.generate_batch_song(\n",
    "            sampled_200_midi, batch_song, start_index=i, fs=frame_per_second, \n",
    "            seq_len=seq_len, use_tqdm=False)\n",
    "print(inputs_nnet_large[0]) #For each timestep we get a set of notes. If the next timestep contains the same note, it is held longer\n",
    "# LOL\n",
    "print(outputs_nnet_large[0])\n",
    "\n",
    "trans_in = note_tokenizer.transform(inputs_nnet_large)\n",
    "trans_out = note_tokenizer.transform(outputs_nnet_large)\n",
    "\n",
    "print((trans_in.shape))\n",
    "print((trans_out.shape))\n",
    "\n",
    "print(\"HEllo\")\n",
    "\n",
    "print(trans_in[0].shape)\n",
    "print(trans_out[1].shape)\n",
    "\n",
    "in_tensor = torch.tensor(trans_in,dtype=torch.float32)\n",
    "in_tensor = in_tensor[None,:,:] # Should be (seq_len, batch, input_size)\n",
    "in_tensor = in_tensor.permute(1,0,2)\n",
    "print(in_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36540, 1])\n",
      "torch.Size([2085, 1, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[0, 0, 0]' is invalid for input of size 76185900",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-803618d84084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mout_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[0, 0, 0]' is invalid for input of size 76185900"
     ]
    }
   ],
   "source": [
    "## Training loop\n",
    "from torch import optim\n",
    "model = our_LSTM(h0,c0)\n",
    "\n",
    "#out, h, c = model(in_tensor, h0, c0) # Expects input with a batch dimension.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print((out.shape))\n",
    "\n",
    "##Train\n",
    "\n",
    "for epoch in range(0,10):\n",
    "    batch_song = BATCH_SONG\n",
    "    frame_per_second = FRAME_PER_SECOND\n",
    "    inputs_nnet_large, outputs_nnet_large = token.generate_batch_song(\n",
    "            sampled_200_midi, batch_song, start_index=i, fs=frame_per_second, \n",
    "            seq_len=seq_len, use_tqdm=False)\n",
    "\n",
    "    trans_in = note_tokenizer.transform(inputs_nnet_large)\n",
    "    trans_out = note_tokenizer.transform(outputs_nnet_large)\n",
    "    in_tensor = torch.tensor(trans_in,dtype=torch.float32)\n",
    "    in_tensor = in_tensor[None,:,:] # Should be (seq_len, batch, input_size)\n",
    "    in_tensor = in_tensor.permute(1,0,2)\n",
    "    print(in_tensor.shape)\n",
    "    out_tensor = torch.tensor(trans_out,dtype=torch.float32)\n",
    "    pred, _, _ = model(in_tensor,h0,c0)\n",
    "    print(pred.view(0,0,0).shape)\n",
    "    print(out_tensor)\n",
    "    loss = loss_fn(pred,out_tensor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "data = out[0][:][:].data[:]\n",
    "print(data.T.shape)\n",
    "data2 = data.T[:]\n",
    "print(min(data2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

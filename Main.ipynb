{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for creating and training the LSTM network models used to generate classical music.\n",
    "\n",
    "Matilda Wikström\n",
    "\n",
    "Jesper Larsson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to good stuff!\n",
    "\n",
    "https://github.com/craffel/pretty-midi\n",
    "\n",
    "https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "# \"Our code\"\n",
    "import preprocessing as pp\n",
    "#import generatemidi as gm # Broken file, fix\n",
    "import tokenizer as token\n",
    "import generatemidi as gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29070\n"
     ]
    }
   ],
   "source": [
    "# Using pretty_midi example\n",
    "fs=30\n",
    "dict_note = {}\n",
    "midi_pretty_format = pretty_midi.PrettyMIDI('test.midi')\n",
    "piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
    "piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
    "dict_note[0] = piano_roll\n",
    "\n",
    "print(len(piano_roll[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the required data create folder structure\n",
    "#Make sure we are in the correct folder\n",
    "\n",
    "assert (os.path.basename(os.getcwd())=='LSTM-MusicGenerator'), \"Wrong working dir\"\n",
    "\n",
    "#Download the MAESTRO Dataset\n",
    "if not os.path.isfile('./maestro-v1.0.0-midi.zip'):\n",
    "    !wget https://storage.googleapis.com/magentadata/datasets/maestro/v1.0.0/maestro-v1.0.0-midi.zip\n",
    "\n",
    "#Check if we have extracted the files\n",
    "if not os.path.isdir('./maestro-v1.0.0-midi') and not os.path.isfile('./maestro-v1.0.0/LICENSE'):\n",
    "    !unzip maestro-v1.0.0-midi.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4011d634d96247679ac6dd998d879038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "list_all_midi = pp.get_list_midi() # \n",
    "sampled_200_midi = list_all_midi[0:100]\n",
    "batch = 1\n",
    "start_index = 0\n",
    "note_tokenizer = token.NoteTokenizer()\n",
    "import pretty_midi\n",
    "\n",
    "for i in tqdm_notebook(range(len(sampled_200_midi))):\n",
    "    dict_time_notes = token.generate_dict_time_notes(sampled_200_midi, batch_song=1, start_index=i, use_tqdm=False, fs=5)\n",
    "    full_notes = token.process_notes_in_song(dict_time_notes)\n",
    "    for note in full_notes:\n",
    "        note_tokenizer.partial_fit(list(note.values()))\n",
    "note_tokenizer.add_new_note('e') # Add empty notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "\n",
    "#unique_notes+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Input dimension = 50\n",
    "#Batch dimension = ? May be one, since we are putting them on \n",
    "#Hidden size, try different versions\n",
    "   \n",
    "input_size = 50\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "is_bidirectional = False\n",
    "#dropout_rate = 0.75\n",
    "dropout_rate = 0\n",
    "batch_size = 1 #96 ### Should be 2085\n",
    "\n",
    "seq_len = 50\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 16\n",
    "BATCH_NNET_SIZE = 96 \n",
    "TOTAL_SONGS = len(sampled_200_midi)\n",
    "FRAME_PER_SECOND = 5\n",
    "\n",
    "\n",
    "unique_notes = note_tokenizer.unique_word #Used in our output layer to map\n",
    "\n",
    "\n",
    "\n",
    "if is_bidirectional:\n",
    "    num_directions = 2\n",
    "else:\n",
    "    num_directions = 1\n",
    "\n",
    "input = torch.randn(1, batch_size, input_size)\n",
    "h0 = torch.randn(num_layers*num_directions, batch_size, hidden_size,dtype=torch.float32)\n",
    "c0 = torch.randn(num_layers*num_directions, batch_size, hidden_size,dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "class our_LSTM(torch.nn.Module):\n",
    "    def __init__(self, h0, c0):\n",
    "        super().__init__()\n",
    "        self.hn = h0\n",
    "        self.cn = c0\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout_rate, bidirectional=is_bidirectional)\n",
    "        self.fc = torch.nn.Linear(num_directions * hidden_size, unique_notes)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out, (self.hn, self.cn) = self.lstm(inp, (self.hn, self.cn))\n",
    "        out = self.fc(out)\n",
    "        #out = self.softmax(out)\n",
    "        out = out.permute(0,2,1)\n",
    "        out = out.view(-1,unique_notes)\n",
    "        \n",
    "        return out, self.hn, self.cn\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T RUN THIS CELL ##\n",
    "\n",
    "seq_len = 50\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 16\n",
    "BATCH_NNET_SIZE = 96\n",
    "TOTAL_SONGS = len(sampled_200_midi)\n",
    "FRAME_PER_SECOND = 5\n",
    "\n",
    "batch_song = BATCH_SONG\n",
    "frame_per_second = FRAME_PER_SECOND\n",
    "inputs_nnet_large, outputs_nnet_large = token.generate_batch_song(\n",
    "            sampled_200_midi, batch_song, start_index=i, fs=frame_per_second, \n",
    "            seq_len=seq_len, use_tqdm=False)\n",
    "print(inputs_nnet_large[0]) #For each timestep we get a set of notes. If the next timestep contains the same note, it is held longer\n",
    "# LOL\n",
    "print(outputs_nnet_large[0])\n",
    "\n",
    "trans_in = note_tokenizer.transform(inputs_nnet_large)\n",
    "trans_out = note_tokenizer.transform(outputs_nnet_large)\n",
    "\n",
    "print((trans_in.shape))\n",
    "print((trans_out.shape))\n",
    "\n",
    "print(\"HEllo\")\n",
    "\n",
    "print(trans_in[0].shape)\n",
    "print(trans_out[1].shape)\n",
    "\n",
    "in_tensor = torch.tensor(trans_in,dtype=torch.float32)\n",
    "in_tensor = in_tensor[None,:,:] # Should be (seq_len, batch, input_size)\n",
    "in_tensor = in_tensor.permute(1,0,2)\n",
    "print(in_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f869f21638a046a7b14409e897a7e2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='i', max=7, style=ProgressStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693a79c596214b6e8b00e8b568f5b958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='j', max=517, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1103, grad_fn=<DivBackward0>)\n",
      "tensor(0.1101, grad_fn=<DivBackward0>)\n",
      "tensor(0.1105, grad_fn=<DivBackward0>)\n",
      "tensor(0.1103, grad_fn=<DivBackward0>)\n",
      "tensor(0.1098, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1103, grad_fn=<DivBackward0>)\n",
      "tensor(0.1099, grad_fn=<DivBackward0>)\n",
      "tensor(0.1101, grad_fn=<DivBackward0>)\n",
      "tensor(0.1102, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1106, grad_fn=<DivBackward0>)\n",
      "tensor(0.1102, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1104, grad_fn=<DivBackward0>)\n",
      "tensor(0.1097, grad_fn=<DivBackward0>)\n",
      "tensor(0.1101, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1099, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1097, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1100, grad_fn=<DivBackward0>)\n",
      "tensor(0.1093, grad_fn=<DivBackward0>)\n",
      "tensor(0.1098, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1098, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1099, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1099, grad_fn=<DivBackward0>)\n",
      "tensor(0.1095, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1095, grad_fn=<DivBackward0>)\n",
      "tensor(0.1090, grad_fn=<DivBackward0>)\n",
      "tensor(0.1087, grad_fn=<DivBackward0>)\n",
      "tensor(0.1095, grad_fn=<DivBackward0>)\n",
      "tensor(0.1101, grad_fn=<DivBackward0>)\n",
      "tensor(0.1095, grad_fn=<DivBackward0>)\n",
      "tensor(0.1095, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1092, grad_fn=<DivBackward0>)\n",
      "tensor(0.1093, grad_fn=<DivBackward0>)\n",
      "tensor(0.1091, grad_fn=<DivBackward0>)\n",
      "tensor(0.1093, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1097, grad_fn=<DivBackward0>)\n",
      "tensor(0.1092, grad_fn=<DivBackward0>)\n",
      "tensor(0.1089, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1090, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "tensor(0.1093, grad_fn=<DivBackward0>)\n",
      "tensor(0.1097, grad_fn=<DivBackward0>)\n",
      "tensor(0.1087, grad_fn=<DivBackward0>)\n",
      "tensor(0.1093, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1077, grad_fn=<DivBackward0>)\n",
      "tensor(0.1083, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1087, grad_fn=<DivBackward0>)\n",
      "tensor(0.1087, grad_fn=<DivBackward0>)\n",
      "tensor(0.1091, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1096, grad_fn=<DivBackward0>)\n",
      "tensor(0.1089, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1077, grad_fn=<DivBackward0>)\n",
      "tensor(0.1074, grad_fn=<DivBackward0>)\n",
      "tensor(0.1072, grad_fn=<DivBackward0>)\n",
      "tensor(0.1069, grad_fn=<DivBackward0>)\n",
      "tensor(0.1075, grad_fn=<DivBackward0>)\n",
      "tensor(0.1082, grad_fn=<DivBackward0>)\n",
      "tensor(0.1071, grad_fn=<DivBackward0>)\n",
      "tensor(0.1076, grad_fn=<DivBackward0>)\n",
      "tensor(0.1075, grad_fn=<DivBackward0>)\n",
      "tensor(0.1070, grad_fn=<DivBackward0>)\n",
      "tensor(0.1077, grad_fn=<DivBackward0>)\n",
      "tensor(0.1075, grad_fn=<DivBackward0>)\n",
      "tensor(0.1070, grad_fn=<DivBackward0>)\n",
      "tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "tensor(0.1078, grad_fn=<DivBackward0>)\n",
      "tensor(0.1073, grad_fn=<DivBackward0>)\n",
      "tensor(0.1076, grad_fn=<DivBackward0>)\n",
      "tensor(0.1066, grad_fn=<DivBackward0>)\n",
      "tensor(0.1068, grad_fn=<DivBackward0>)\n",
      "tensor(0.1069, grad_fn=<DivBackward0>)\n",
      "tensor(0.1065, grad_fn=<DivBackward0>)\n",
      "tensor(0.1075, grad_fn=<DivBackward0>)\n",
      "tensor(0.1073, grad_fn=<DivBackward0>)\n",
      "tensor(0.1078, grad_fn=<DivBackward0>)\n",
      "tensor(0.1087, grad_fn=<DivBackward0>)\n",
      "tensor(0.1083, grad_fn=<DivBackward0>)\n",
      "tensor(0.1078, grad_fn=<DivBackward0>)\n",
      "tensor(0.1081, grad_fn=<DivBackward0>)\n",
      "tensor(0.1090, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1084, grad_fn=<DivBackward0>)\n",
      "tensor(0.1078, grad_fn=<DivBackward0>)\n",
      "tensor(0.1066, grad_fn=<DivBackward0>)\n",
      "tensor(0.1082, grad_fn=<DivBackward0>)\n",
      "tensor(0.1071, grad_fn=<DivBackward0>)\n",
      "tensor(0.1076, grad_fn=<DivBackward0>)\n",
      "tensor(0.1087, grad_fn=<DivBackward0>)\n",
      "tensor(0.1077, grad_fn=<DivBackward0>)\n",
      "tensor(0.1076, grad_fn=<DivBackward0>)\n",
      "tensor(0.1072, grad_fn=<DivBackward0>)\n",
      "tensor(0.1068, grad_fn=<DivBackward0>)\n",
      "tensor(0.1050, grad_fn=<DivBackward0>)\n",
      "tensor(0.1060, grad_fn=<DivBackward0>)\n",
      "tensor(0.1072, grad_fn=<DivBackward0>)\n",
      "tensor(0.1043, grad_fn=<DivBackward0>)\n",
      "tensor(0.1051, grad_fn=<DivBackward0>)\n",
      "tensor(0.1062, grad_fn=<DivBackward0>)\n",
      "tensor(0.1063, grad_fn=<DivBackward0>)\n",
      "tensor(0.1063, grad_fn=<DivBackward0>)\n",
      "tensor(0.1065, grad_fn=<DivBackward0>)\n",
      "tensor(0.1069, grad_fn=<DivBackward0>)\n",
      "tensor(0.1063, grad_fn=<DivBackward0>)\n",
      "tensor(0.1082, grad_fn=<DivBackward0>)\n",
      "tensor(0.1082, grad_fn=<DivBackward0>)\n",
      "tensor(0.1062, grad_fn=<DivBackward0>)\n",
      "tensor(0.1073, grad_fn=<DivBackward0>)\n",
      "tensor(0.1075, grad_fn=<DivBackward0>)\n",
      "tensor(0.1079, grad_fn=<DivBackward0>)\n",
      "tensor(0.1071, grad_fn=<DivBackward0>)\n",
      "tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "tensor(0.1058, grad_fn=<DivBackward0>)\n",
      "tensor(0.1054, grad_fn=<DivBackward0>)\n",
      "tensor(0.1067, grad_fn=<DivBackward0>)\n",
      "tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "tensor(0.1058, grad_fn=<DivBackward0>)\n",
      "tensor(0.1058, grad_fn=<DivBackward0>)\n",
      "tensor(0.1048, grad_fn=<DivBackward0>)\n",
      "tensor(0.1034, grad_fn=<DivBackward0>)\n",
      "tensor(0.1051, grad_fn=<DivBackward0>)\n",
      "tensor(0.1035, grad_fn=<DivBackward0>)\n",
      "tensor(0.1031, grad_fn=<DivBackward0>)\n",
      "tensor(0.1077, grad_fn=<DivBackward0>)\n",
      "tensor(0.1053, grad_fn=<DivBackward0>)\n",
      "tensor(0.1053, grad_fn=<DivBackward0>)\n",
      "tensor(0.1054, grad_fn=<DivBackward0>)\n",
      "tensor(0.1062, grad_fn=<DivBackward0>)\n",
      "tensor(0.1066, grad_fn=<DivBackward0>)\n",
      "tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "tensor(0.1065, grad_fn=<DivBackward0>)\n",
      "tensor(0.1059, grad_fn=<DivBackward0>)\n",
      "tensor(0.1055, grad_fn=<DivBackward0>)\n",
      "tensor(0.1056, grad_fn=<DivBackward0>)\n",
      "tensor(0.1030, grad_fn=<DivBackward0>)\n",
      "tensor(0.1009, grad_fn=<DivBackward0>)\n",
      "tensor(0.1044, grad_fn=<DivBackward0>)\n",
      "tensor(0.1030, grad_fn=<DivBackward0>)\n",
      "tensor(0.1020, grad_fn=<DivBackward0>)\n",
      "tensor(0.1020, grad_fn=<DivBackward0>)\n",
      "tensor(0.1027, grad_fn=<DivBackward0>)\n",
      "tensor(0.1056, grad_fn=<DivBackward0>)\n",
      "tensor(0.1043, grad_fn=<DivBackward0>)\n",
      "tensor(0.1036, grad_fn=<DivBackward0>)\n",
      "tensor(0.1050, grad_fn=<DivBackward0>)\n",
      "tensor(0.1057, grad_fn=<DivBackward0>)\n",
      "tensor(0.1042, grad_fn=<DivBackward0>)\n",
      "tensor(0.1059, grad_fn=<DivBackward0>)\n",
      "tensor(0.1039, grad_fn=<DivBackward0>)\n",
      "tensor(0.1044, grad_fn=<DivBackward0>)\n",
      "tensor(0.1037, grad_fn=<DivBackward0>)\n",
      "tensor(0.1022, grad_fn=<DivBackward0>)\n",
      "tensor(0.0995, grad_fn=<DivBackward0>)\n",
      "tensor(0.1016, grad_fn=<DivBackward0>)\n",
      "tensor(0.1003, grad_fn=<DivBackward0>)\n",
      "tensor(0.1034, grad_fn=<DivBackward0>)\n",
      "tensor(0.1058, grad_fn=<DivBackward0>)\n",
      "tensor(0.1062, grad_fn=<DivBackward0>)\n",
      "tensor(0.1040, grad_fn=<DivBackward0>)\n",
      "tensor(0.1038, grad_fn=<DivBackward0>)\n",
      "tensor(0.1032, grad_fn=<DivBackward0>)\n",
      "tensor(0.1042, grad_fn=<DivBackward0>)\n",
      "tensor(0.1030, grad_fn=<DivBackward0>)\n",
      "tensor(0.1020, grad_fn=<DivBackward0>)\n",
      "tensor(0.1017, grad_fn=<DivBackward0>)\n",
      "tensor(0.1032, grad_fn=<DivBackward0>)\n",
      "tensor(0.1050, grad_fn=<DivBackward0>)\n",
      "tensor(0.1030, grad_fn=<DivBackward0>)\n",
      "tensor(0.0991, grad_fn=<DivBackward0>)\n",
      "tensor(0.1028, grad_fn=<DivBackward0>)\n",
      "tensor(0.0999, grad_fn=<DivBackward0>)\n",
      "tensor(0.1007, grad_fn=<DivBackward0>)\n",
      "tensor(0.1038, grad_fn=<DivBackward0>)\n",
      "tensor(0.1047, grad_fn=<DivBackward0>)\n",
      "tensor(0.1041, grad_fn=<DivBackward0>)\n",
      "tensor(0.1023, grad_fn=<DivBackward0>)\n",
      "tensor(0.1037, grad_fn=<DivBackward0>)\n",
      "tensor(0.1029, grad_fn=<DivBackward0>)\n",
      "tensor(0.1041, grad_fn=<DivBackward0>)\n",
      "tensor(0.1017, grad_fn=<DivBackward0>)\n",
      "tensor(0.0997, grad_fn=<DivBackward0>)\n",
      "tensor(0.0957, grad_fn=<DivBackward0>)\n",
      "tensor(0.0988, grad_fn=<DivBackward0>)\n",
      "tensor(0.0988, grad_fn=<DivBackward0>)\n",
      "tensor(0.0986, grad_fn=<DivBackward0>)\n",
      "tensor(0.1027, grad_fn=<DivBackward0>)\n",
      "tensor(0.1029, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1015, grad_fn=<DivBackward0>)\n",
      "tensor(0.0964, grad_fn=<DivBackward0>)\n",
      "tensor(0.0904, grad_fn=<DivBackward0>)\n",
      "tensor(0.1030, grad_fn=<DivBackward0>)\n",
      "tensor(0.1034, grad_fn=<DivBackward0>)\n",
      "tensor(0.1059, grad_fn=<DivBackward0>)\n",
      "tensor(0.1042, grad_fn=<DivBackward0>)\n",
      "tensor(0.0998, grad_fn=<DivBackward0>)\n",
      "tensor(0.1028, grad_fn=<DivBackward0>)\n",
      "tensor(0.1027, grad_fn=<DivBackward0>)\n",
      "tensor(0.1088, grad_fn=<DivBackward0>)\n",
      "tensor(0.1033, grad_fn=<DivBackward0>)\n",
      "tensor(0.1036, grad_fn=<DivBackward0>)\n",
      "tensor(0.1021, grad_fn=<DivBackward0>)\n",
      "tensor(0.1021, grad_fn=<DivBackward0>)\n",
      "tensor(0.1026, grad_fn=<DivBackward0>)\n",
      "tensor(0.1015, grad_fn=<DivBackward0>)\n",
      "tensor(0.1019, grad_fn=<DivBackward0>)\n",
      "tensor(0.1049, grad_fn=<DivBackward0>)\n",
      "tensor(0.1041, grad_fn=<DivBackward0>)\n",
      "tensor(0.1008, grad_fn=<DivBackward0>)\n",
      "tensor(0.1029, grad_fn=<DivBackward0>)\n",
      "tensor(0.1018, grad_fn=<DivBackward0>)\n",
      "tensor(0.1046, grad_fn=<DivBackward0>)\n",
      "tensor(0.1029, grad_fn=<DivBackward0>)\n",
      "tensor(0.1060, grad_fn=<DivBackward0>)\n",
      "tensor(0.1048, grad_fn=<DivBackward0>)\n",
      "tensor(0.1027, grad_fn=<DivBackward0>)\n",
      "tensor(0.1049, grad_fn=<DivBackward0>)\n",
      "tensor(0.1076, grad_fn=<DivBackward0>)\n",
      "tensor(0.1024, grad_fn=<DivBackward0>)\n",
      "tensor(0.1037, grad_fn=<DivBackward0>)\n",
      "tensor(0.1046, grad_fn=<DivBackward0>)\n",
      "tensor(0.1048, grad_fn=<DivBackward0>)\n",
      "tensor(0.0993, grad_fn=<DivBackward0>)\n",
      "tensor(0.0942, grad_fn=<DivBackward0>)\n",
      "tensor(0.0966, grad_fn=<DivBackward0>)\n",
      "tensor(0.0975, grad_fn=<DivBackward0>)\n",
      "tensor(0.1000, grad_fn=<DivBackward0>)\n",
      "tensor(0.0962, grad_fn=<DivBackward0>)\n",
      "tensor(0.0977, grad_fn=<DivBackward0>)\n",
      "tensor(0.0991, grad_fn=<DivBackward0>)\n",
      "tensor(0.0966, grad_fn=<DivBackward0>)\n",
      "tensor(0.1013, grad_fn=<DivBackward0>)\n",
      "tensor(0.0922, grad_fn=<DivBackward0>)\n",
      "tensor(0.0976, grad_fn=<DivBackward0>)\n",
      "tensor(0.0937, grad_fn=<DivBackward0>)\n",
      "tensor(0.0923, grad_fn=<DivBackward0>)\n",
      "tensor(0.0970, grad_fn=<DivBackward0>)\n",
      "tensor(0.0988, grad_fn=<DivBackward0>)\n",
      "tensor(0.0962, grad_fn=<DivBackward0>)\n",
      "tensor(0.1032, grad_fn=<DivBackward0>)\n",
      "tensor(0.1008, grad_fn=<DivBackward0>)\n",
      "tensor(0.0984, grad_fn=<DivBackward0>)\n",
      "tensor(0.0984, grad_fn=<DivBackward0>)\n",
      "tensor(0.0980, grad_fn=<DivBackward0>)\n",
      "tensor(0.0968, grad_fn=<DivBackward0>)\n",
      "tensor(0.0932, grad_fn=<DivBackward0>)\n",
      "tensor(0.0954, grad_fn=<DivBackward0>)\n",
      "tensor(0.0999, grad_fn=<DivBackward0>)\n",
      "tensor(0.0996, grad_fn=<DivBackward0>)\n",
      "tensor(0.0978, grad_fn=<DivBackward0>)\n",
      "tensor(0.0987, grad_fn=<DivBackward0>)\n",
      "tensor(0.0917, grad_fn=<DivBackward0>)\n",
      "tensor(0.0977, grad_fn=<DivBackward0>)\n",
      "tensor(0.1019, grad_fn=<DivBackward0>)\n",
      "tensor(0.0997, grad_fn=<DivBackward0>)\n",
      "tensor(0.0923, grad_fn=<DivBackward0>)\n",
      "tensor(0.0961, grad_fn=<DivBackward0>)\n",
      "tensor(0.0954, grad_fn=<DivBackward0>)\n",
      "tensor(0.0945, grad_fn=<DivBackward0>)\n",
      "tensor(0.0973, grad_fn=<DivBackward0>)\n",
      "tensor(0.0903, grad_fn=<DivBackward0>)\n",
      "tensor(0.0847, grad_fn=<DivBackward0>)\n",
      "tensor(0.0883, grad_fn=<DivBackward0>)\n",
      "tensor(0.0913, grad_fn=<DivBackward0>)\n",
      "tensor(0.0914, grad_fn=<DivBackward0>)\n",
      "tensor(0.0955, grad_fn=<DivBackward0>)\n",
      "tensor(0.0934, grad_fn=<DivBackward0>)\n",
      "tensor(0.0902, grad_fn=<DivBackward0>)\n",
      "tensor(0.0911, grad_fn=<DivBackward0>)\n",
      "tensor(0.0912, grad_fn=<DivBackward0>)\n",
      "tensor(0.0922, grad_fn=<DivBackward0>)\n",
      "tensor(0.0932, grad_fn=<DivBackward0>)\n",
      "tensor(0.0953, grad_fn=<DivBackward0>)\n",
      "tensor(0.0955, grad_fn=<DivBackward0>)\n",
      "tensor(0.0941, grad_fn=<DivBackward0>)\n",
      "tensor(0.0917, grad_fn=<DivBackward0>)\n",
      "tensor(0.0927, grad_fn=<DivBackward0>)\n",
      "tensor(0.0953, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Training loop\n",
    "seq_len = 50 # input length here.\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 16\n",
    "BATCH_NNET_SIZE = 96 # Our sequence length here.\n",
    "TOTAL_SONGS = len(sampled_200_midi)\n",
    "FRAME_PER_SECOND = 5\n",
    "from torch import optim\n",
    "model = our_LSTM(h0,c0)\n",
    "\n",
    "#out, h, c = model(in_tensor, h0, c0) # Expects input with a batch dimension.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "##Train\n",
    "\n",
    "for epoch in range(0,1):\n",
    "    batch_song = BATCH_SONG\n",
    "    frame_per_second = FRAME_PER_SECOND\n",
    "    shuffle(sampled_200_midi)\n",
    "    for i in tqdm_notebook(range(0,len(sampled_200_midi),BATCH_SONG), desc='i'):\n",
    "        #print(\"i=:\",i)\n",
    "        inputs_nnet_large, outputs_nnet_large = token.generate_batch_song(\n",
    "                sampled_200_midi, batch_song, start_index=i, fs=frame_per_second, \n",
    "                seq_len=seq_len, use_tqdm=False)\n",
    "\n",
    "        trans_in = note_tokenizer.transform(inputs_nnet_large)\n",
    "        trans_out = note_tokenizer.transform(outputs_nnet_large)\n",
    "        #print(\"Max of out: \", max(trans_out), \"Min of out :\", min(trans_out))\n",
    "        trans_out = trans_out - 1\n",
    "        for j in tqdm_notebook(range(0,len(trans_in)-BATCH_NNET_SIZE,BATCH_NNET_SIZE), desc='j'):\n",
    "            #print(\"j=:\",j)\n",
    "            #Want to iterate over the sequence in steps of 96\n",
    "            input_tensor = trans_in[j:j+BATCH_NNET_SIZE]\n",
    "            target_tensor = trans_out[j:j+BATCH_NNET_SIZE]\n",
    "            #print(\"Max of target tensor: \", max(target_tensor), \"Min of target tensor: \", min(target_tensor))\n",
    "\n",
    "            input_tensor = torch.tensor(input_tensor,dtype=torch.float32)\n",
    "            input_tensor = input_tensor[None,:,:] # Should be (seq_len, batch, input_size)\n",
    "            input_tensor = input_tensor.permute(1,0,2)\n",
    "            #print(input_tensor.shape)\n",
    "            target_tensor = torch.tensor(target_tensor,dtype=torch.long).view(-1)\n",
    "            pred, _, _ = model(input_tensor)\n",
    "            #print(\"Pred shape: \", pred.shape)\n",
    "            #print(\"Target shape: \", target_tensor.shape)\n",
    "            loss = loss_fn(pred,target_tensor)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(loss/96)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate music using the network:\n",
    "import generatemidi as gen\n",
    "import numpy as np\n",
    "max_generate = 200\n",
    "unique_notes = note_tokenizer.unique_word\n",
    "seq_len=50\n",
    "generate = gen.generate_from_random(unique_notes, seq_len)\n",
    "generate = gen.generate_notes(generate, model, unique_notes, max_generate, seq_len)\n",
    "gen.write_midi_file_from_generated(generate, \"random.mid\", start_index=seq_len-1, fs=7, max_generated = max_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_random(unique_notes, seq_len=50):\n",
    "  generate = np.random.randint(0,unique_notes,seq_len).tolist()\n",
    "  return generate\n",
    "    \n",
    "def generate_from_one_note(note_tokenizer, new_notes='35'):\n",
    "  generate = [note_tokenizer.notes_to_index['e'] for i in range(49)]\n",
    "  generate += [note_tokenizer.notes_to_index[new_notes]]\n",
    "  return generate\n",
    "\n",
    "def generate_notes(generate, model, unique_notes, max_generated=1000, seq_len=50):\n",
    "  for i in tqdm_notebook(range(max_generated), desc='genrt'):\n",
    "    test_input = np.array([generate])[:,i:i+seq_len]\n",
    "    test_input = torch.tensor(test_input, dtype=torch.float32)\n",
    "    test_inputa = test_input[None,:,:]\n",
    "    #print(test_inputa.shape)\n",
    "    #print(test_inputa)\n",
    "    predicted_note, _, _ = model(test_inputa)\n",
    "    #random_note_pred = np.random.choice(unique_notes+1, 1, replace=False, p=predicted_note[0])\n",
    "    #generate.append(random_note_pred[0])\n",
    "    #print(predicted_note.shape)\n",
    "    generate.append(predicted_note.argmax(dim=1).item())\n",
    "  return generate\n",
    "\n",
    "\n",
    "def write_midi_file_from_generated(generate, midi_file_name = \"result.mid\", start_index=49, fs=8, max_generated=1000):\n",
    " note_string = [note_tokenizer.index_to_notes[ind_note] for ind_note in generate]\n",
    " array_piano_roll = np.zeros((128,max_generated+1), dtype=np.int16)\n",
    " for index, note in enumerate(note_string[start_index:]):\n",
    "   if note == 'e':\n",
    "     pass\n",
    "   else:\n",
    "     splitted_note = note.split(',')\n",
    "     for j in splitted_note:\n",
    "       array_piano_roll[int(j),index] = 1\n",
    " generate_to_midi = pp.piano_roll_to_pretty_midi(array_piano_roll, fs=fs)\n",
    " print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n",
    " for note in generate_to_midi.instruments[0].notes:\n",
    "   note.velocity = 100\n",
    " generate_to_midi.write(midi_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_generate = 200\n",
    "unique_notes = note_tokenizer.unique_word\n",
    "seq_len=50\n",
    "generate = generate_from_random(unique_notes, seq_len)\n",
    "print(generate)\n",
    "generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)\n",
    "print(generate)\n",
    "write_midi_file_from_generated(generate, \"random2.mid\", start_index=seq_len-1, fs=7, max_generated = max_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
